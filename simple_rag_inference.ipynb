{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load \"Vector DB\" and run RAG query on pre-trained instruct LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import pickle\n",
    "import sys\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextIteratorStreamer,\n",
    "    TextStreamer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load chunk contents and embedding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/lincoln_chunks.pkl\", \"rb\") as file:\n",
    "    chunks = pickle.load(file)\n",
    "\n",
    "index = faiss.read_index(\"data/lincoln_chunks.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 153)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks), index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM to run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7217f58c11447a08ef276024766ca69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c6833924c84f0e8148608c8091913f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747bd1fce3204d2b9b8732d5f3e4916b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afd844761e34231ab459b533abd754d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19e70fc53bb47718b99dbf90eeb935b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b7956b2999465999d02cb8d1b7147f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8365c34b5b17419e93f73d23560a2d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd886535731484aa788bb74ec81555a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50db20eda7f84e22922676a1b76e9889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c5c7b71966412da5287c468e5cfe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5365189d9b4ddd85d4a0514dc34c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd603ee9cda448a991dc8a95493c877d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d7ea8800d04344a55e619bde7003c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# query_model = \"google/gemma-2b-it\"\n",
    "query_model = \"google/gemma-7b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(query_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    query_model, torch_dtype=torch.bfloat16, device_map=\"mps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct RAG Query and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_streamed(query, query_model):\n",
    "    system_prompt = \"You are a helpful assistant who answers question truthfully to the best of your knowledge. You decline to answer if you do not know the answer.\"\n",
    "\n",
    "    chat = [\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": system_prompt,\n",
    "        # },\n",
    "        \n",
    "        {\n",
    "\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{system_prompt}\\n\\n{query}\",  # Pick up the book\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "    streamer = TextStreamer(\n",
    "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder = SentenceTransformer(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_chunks(query: str, k=1) -> str:\n",
    "    \"\"\"\n",
    "    Find closest chunk for a given query.\n",
    "    \"\"\"\n",
    "    embeddings = query_encoder.encode([query])\n",
    "    D, I = index.search(embeddings, k)\n",
    "    return D, I\n",
    "\n",
    "\n",
    "def run_rag_query_streamed(query, query_model, k=3):\n",
    "    # Retrieve most similar chunks\n",
    "    D, I = top_k_chunks(query, k=k)\n",
    "    # formatted_chunks = '\\n\\n'.join([\"Document: \" + chunks[i] for i in I[0]])\n",
    "    formatted_chunks = ' '.join([chunks[i] for i in I[0]])\n",
    "    \n",
    "    # rag_query = f\"Answer the query below and ground your answer in facts contained in the documents below:\\n\\nQuery: {query}\\n\\n{formatted_chunks}\"\n",
    "\n",
    "    rag_query = f\"{formatted_chunks}\\n\\nAnswer the following question: {query}\"\n",
    "\n",
    "    print(rag_query)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    run_query_streamed(rag_query, query_model)\n",
    "\n",
    "    # for i, d in zip(I[0], D[0]):\n",
    "    #     print(d, chunks[i])\n",
    "    #     print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Examples\n",
    "Compare answers to questions, with and without context from most similar document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemma-2b-it\n",
      "I "
     ]
    }
   ],
   "source": [
    "query = \"Why did Abraham Lincoln grow a beard?\"\n",
    "\n",
    "print(\"google/gemma-2b-it\")\n",
    "run_query_streamed(query, query_model)\n",
    "print()\n",
    "\n",
    "print(\"google/gemma-2b-it + RAG\")\n",
    "run_rag_query_streamed(query, query_model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
