{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load \"Vector DB\" and run RAG query on pre-trained instruct LLM\n",
    "Simply appends retrieved chunks to query. No fancy pipeline steps or dedicated Vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from threading import Thread\n",
    "import time\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextIteratorStreamer,\n",
    "    TextStreamer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load chunk contents and embedding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Database for chunks\n",
    "index = faiss.read_index(\"wikipedia-en.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49522046"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM to run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0.dev20240829+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28+d444815.d20240829. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:209: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:210: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:211: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "# query_model = \"google/gemma-2b-it\"\n",
    "query_model = \"/home/stefanwebb/models/llms/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/stefanwebb/models/llms/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct RAG Query and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_streamed(query, query_model):\n",
    "    system_prompt = \"You are a helpful assistant who answers question truthfully to the best of your knowledge. You decline to answer if you do not know the answer.\"\n",
    "\n",
    "    chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        \n",
    "        {\n",
    "\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{query}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    streamer = TextStreamer(\n",
    "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder = SentenceTransformer(\"/home/stefanwebb/models/llms/multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_chunks(query: str, k=1) -> str:\n",
    "    \"\"\"\n",
    "    Find closest chunk for a given query.\n",
    "    \"\"\"\n",
    "    embeddings = query_encoder.encode([query])\n",
    "    D, I = index.search(embeddings, k)\n",
    "    return D, I\n",
    "\n",
    "\n",
    "def run_rag_query_streamed(query, query_model, k=3):\n",
    "    # Retrieve most similar chunks\n",
    "    D, I = top_k_chunks(query, k=k)\n",
    "    # formatted_chunks = '\\n\\n'.join([\"Document: \" + chunks[i] for i in I[0]])\n",
    "    formatted_chunks = ' '.join([chunks[i] for i in I[0]])\n",
    "    \n",
    "    # rag_query = f\"Answer the query below and ground your answer in facts contained in the documents below:\\n\\nQuery: {query}\\n\\n{formatted_chunks}\"\n",
    "\n",
    "    rag_query = f\"{formatted_chunks}\\n\\nAnswer the following question: {query}\"\n",
    "\n",
    "    # DEBUG\n",
    "    # print(rag_query)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    run_query_streamed(rag_query, query_model)\n",
    "\n",
    "    # for i, d in zip(I[0], D[0]):\n",
    "    #     print(d, chunks[i])\n",
    "    #     print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Examples\n",
    "Compare answers to questions, with and without context from most similar document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "query = \"Why did Abraham Lincoln grow a beard?\"\n",
    "embeddings = query_encoder.encode([query])\n",
    "D, I = index.search(embeddings, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40692232, 47828934, 13314770])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral-7B-Instruct-v0.3\n",
      "Abraham Lincoln grew a beard primarily for practical reasons. In the mid-19th century, beards were common among men, but Lincoln did not have a beard during his first term as a U.S. Representative. After losing the Senate race in 1858, he grew a beard as a way to differentiate himself from his opponents in his presidential campaign in 1860. The beard also helped to hide a condition he had called \"tuberculosis laryngitis,\" which caused his voice to be weak and hoarse. Additionally, the beard may have helped to make him appear more mature and serious, which could have been beneficial in a time of national crisis.\n",
      "\n",
      "Mistral-7B-Instruct-v0.3 + RAG\n",
      "Abraham Lincoln grew a beard in 1860, at the suggestion of 11-year-old Grace Bedell, who wrote him a letter advising that a beard would improve his appearance. He was the first of five presidents to do so. This decision was not only influenced by the advice of a young girl but also as a political strategy, as it helped to project a more mature and serious image during his presidential campaign.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Why did Abraham Lincoln grow a beard?\"\n",
    "\n",
    "# print(\"Mistral-7B-Instruct-v0.3\")\n",
    "# run_query_streamed(query, model)\n",
    "# print()\n",
    "\n",
    "# print(\"Mistral-7B-Instruct-v0.3 + RAG\")\n",
    "# run_rag_query_streamed(query, model)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = type(tokenizer(\"Hello\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
