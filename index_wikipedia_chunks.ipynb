{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Wikipedia Chunk Embeddings\n",
    "* How much can I scale indexing on my desktop computer?\n",
    "* Using 64GB system memory and 24GB GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'\n",
    "import pickle\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_path = '/home/stefanwebb/data/wikimedia/wikipedia/20231101.en'\n",
    "embeddings_path = '/home/stefanwebb/embeddings/wikimedia/wikipedia/20231101.en'\n",
    "files = [f\"train-{idx:05d}-of-00041.parquet\" for idx in range(41)]\n",
    "batch_size = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count How Many Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 6407814\n"
     ]
    }
   ],
   "source": [
    "count_documents = 0\n",
    "for file in files:\n",
    "    fullpath = os.path.join(documents_path, file)\n",
    "    parquet_file = pq.ParquetFile(fullpath)\n",
    "    count_documents += parquet_file.metadata.num_rows\n",
    "print('Documents:', count_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count How Many Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks 49522046\n"
     ]
    }
   ],
   "source": [
    "count_chunks = 0\n",
    "for file in files:\n",
    "    fullpath = os.path.join(embeddings_path, file)\n",
    "    parquet_file = pq.ParquetFile(fullpath)\n",
    "    count_chunks += parquet_file.metadata.num_rows\n",
    "print('Chunks', count_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg chunks/doc: 7.73\n"
     ]
    }
   ],
   "source": [
    "print('Avg chunks/doc:', round(count_chunks/count_documents, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Requirements\n",
    "* We clearly can't keep raw embeddings in 64GB system RAM (although my motherboard can hold up 128GB)\n",
    "* How about we deal with the text of the chunks separately and cast the embeddings to float16?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory for raw embeddings: 76.07 GB\n"
     ]
    }
   ],
   "source": [
    "bytes_per_embedding = 384 * 4\n",
    "print('Memory for raw embeddings:', round(bytes_per_embedding * count_chunks / 10**9, 2), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Embeddings and Chunks\n",
    "* Taking an incremental approach, which is easier for debugging.\n",
    "* First, extract and concatenate the embeddings from each Parquet file.\n",
    "* Separately, save the document chunks to another file.\n",
    "* This step takes about 3 hours on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file in enumerate(files):\n",
    "    print(f\"File {idx} of 41\")\n",
    "    embeddings = []\n",
    "    chunks = []\n",
    "\n",
    "    fullpath = os.path.join(embeddings_path, file)\n",
    "    dataset = load_dataset(\"parquet\", data_files={'train': fullpath}, streaming=True, batch_size=batch_size)\n",
    "\n",
    "    for x in dataset['train'].iter(batch_size=batch_size):\n",
    "        this_embeddings = np.array(x['embeddings']).astype(np.float16)\n",
    "        this_chunks = (x['chunks'])\n",
    "\n",
    "        embeddings.append(this_embeddings)\n",
    "        chunks.extend(this_chunks)\n",
    "\n",
    "    embeddings_file = os.path.join(embeddings_path, f'embeddings-{idx:05d}-of-00041.npy')\n",
    "    with open(embeddings_file, 'wb') as f:\n",
    "        embedding_matrix = np.concatenate(embeddings, axis=0)\n",
    "        np.save(f, embedding_matrix)\n",
    "\n",
    "    chunks_file = os.path.join(embeddings_path, f'chunks-{idx:05d}-of-00041.pkl')\n",
    "    with open(chunks_file, 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    del embedding_matrix\n",
    "    del embeddings\n",
    "    del chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 10% of data\n",
    "* Need a smaller subset for training FAISS index on GPU\n",
    "* This kept crashing due to an out of memory error, so I've moved to standalone script `subsample_embeddings.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = os.path.join(embeddings_path, f'subset-embeddings.npy')\n",
    "Xsubset = np.load(embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4952183, 384), 10.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsubset.shape, round(Xsubset.shape[0] / count_chunks * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train FAISS Index\n",
    "* 10% from each file should be representative of entire dataset\n",
    "* TODO: Calculate maximum fraction of data I can fit into GPU memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert faiss.get_num_gpus() > 0\n",
    "\n",
    "# This type of index is recommended in the FAISS docs for our scale of number of embeddings\n",
    "embed_dim = 384\n",
    "# index = faiss.index_factory(embed_dim, \"IVF16384_HNSW32,Flat\")\n",
    "# index = faiss.index_factory(embed_dim, \"PCA128,IVF16384_HNSW32,Flat\")\n",
    "\n",
    "\n",
    "n_ivf = int(8 * np.sqrt(Xsubset.shape[0]))\n",
    "index = faiss.index_factory(embed_dim, f'PCA64,IVF{n_ivf},Flat', faiss.METRIC_L2)\n",
    "\n",
    "index_ivf = faiss.extract_index_ivf(index)\n",
    "clustering_index = faiss.index_cpu_to_all_gpus(faiss.IndexFlatL2(index_ivf.d))\n",
    "index_ivf.clustering_index = clustering_index\n",
    "index.train(Xsubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ivf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"wikipedia-en-simplifying-empty.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Embeddings to Index\n",
    "* Index file ends up being `13GB`, which is not bad considering the size of the raw embeddings is much larger!\n",
    "* Also, this fits into GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding embeddings to index...\n",
      "File 0 of 41\n",
      "File 1 of 41\n",
      "File 2 of 41\n",
      "File 3 of 41\n",
      "File 4 of 41\n",
      "File 5 of 41\n",
      "File 6 of 41\n",
      "File 7 of 41\n",
      "File 8 of 41\n",
      "File 9 of 41\n",
      "File 10 of 41\n",
      "File 11 of 41\n",
      "File 12 of 41\n",
      "File 13 of 41\n",
      "File 14 of 41\n",
      "File 15 of 41\n",
      "File 16 of 41\n",
      "File 17 of 41\n",
      "File 18 of 41\n",
      "File 19 of 41\n",
      "File 20 of 41\n",
      "File 21 of 41\n",
      "File 22 of 41\n",
      "File 23 of 41\n",
      "File 24 of 41\n",
      "File 25 of 41\n",
      "File 26 of 41\n",
      "File 27 of 41\n",
      "File 28 of 41\n",
      "File 29 of 41\n",
      "File 30 of 41\n",
      "File 31 of 41\n",
      "File 32 of 41\n",
      "File 33 of 41\n",
      "File 34 of 41\n",
      "File 35 of 41\n",
      "File 36 of 41\n",
      "File 37 of 41\n",
      "File 38 of 41\n",
      "File 39 of 41\n",
      "File 40 of 41\n"
     ]
    }
   ],
   "source": [
    "print('Adding embeddings to index...')\n",
    "for idx, file in enumerate(files):\n",
    "    print(f\"File {idx} of 41\")\n",
    "    embeddings_file = os.path.join(embeddings_path, f'embeddings-{idx:05d}-of-00041.npy')\n",
    "    X = np.load(embeddings_file)\n",
    "    index.add(X)\n",
    "    del X\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"wikipedia-en-simplifying.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fin\n",
    "We have a scalable index over all of Wikipedia we can use for RAG inference!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
