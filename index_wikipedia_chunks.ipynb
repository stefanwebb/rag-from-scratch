{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting to Index Wikipedia Chunk Embeddings\n",
    "* How much can I scale indexing on my desktop computer?\n",
    "* Using 64GB system memory and 24GB GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'\n",
    "import pickle\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_path = '/home/stefanwebb/data/wikimedia/wikipedia/20231101.en'\n",
    "embeddings_path = '/home/stefanwebb/embeddings/wikimedia/wikipedia/20231101.en'\n",
    "files = [f\"train-{idx:05d}-of-00041.parquet\" for idx in range(41)]\n",
    "batch_size = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count How Many Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 6407814\n"
     ]
    }
   ],
   "source": [
    "count_documents = 0\n",
    "for file in files:\n",
    "    fullpath = os.path.join(documents_path, file)\n",
    "    parquet_file = pq.ParquetFile(fullpath)\n",
    "    count_documents += parquet_file.metadata.num_rows\n",
    "print('Documents:', count_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count How Many Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks 49522046\n"
     ]
    }
   ],
   "source": [
    "count_chunks = 0\n",
    "for file in files:\n",
    "    fullpath = os.path.join(embeddings_path, file)\n",
    "    parquet_file = pq.ParquetFile(fullpath)\n",
    "    count_chunks += parquet_file.metadata.num_rows\n",
    "print('Chunks', count_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg chunks/doc: 7.73\n"
     ]
    }
   ],
   "source": [
    "print('Avg chunks/doc:', round(count_chunks/count_documents, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Requirements\n",
    "* We clearly can't keep raw embeddings in 64GB system RAM (although my motherboard can hold up 128GB)\n",
    "* How about we deal with the text of the chunks separately and cast the embeddings to float16?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory for raw embeddings: 76.07 GB\n"
     ]
    }
   ],
   "source": [
    "bytes_per_embedding = 384 * 4\n",
    "print('Memory for raw embeddings:', round(bytes_per_embedding * count_chunks / 10**9, 2), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Embeddings and Chunks\n",
    "* Taking an incremental approach, which is easier for debugging.\n",
    "* First, extract and concatenate the embeddings from each Parquet file.\n",
    "* Separately, save the document chunks to another file.\n",
    "* This step takes about 3 hours on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file in enumerate(files):\n",
    "    print(f\"File {idx} of 41\")\n",
    "    embeddings = []\n",
    "    chunks = []\n",
    "\n",
    "    fullpath = os.path.join(embeddings_path, file)\n",
    "    dataset = load_dataset(\"parquet\", data_files={'train': fullpath}, streaming=True, batch_size=batch_size)\n",
    "\n",
    "    for x in dataset['train'].iter(batch_size=batch_size):\n",
    "        this_embeddings = np.array(x['embeddings']).astype(np.float16)\n",
    "        this_chunks = (x['chunks'])\n",
    "\n",
    "        embeddings.append(this_embeddings)\n",
    "        chunks.extend(this_chunks)\n",
    "\n",
    "    embeddings_file = os.path.join(embeddings_path, f'embeddings-{idx:05d}-of-00041.npy')\n",
    "    with open(embeddings_file, 'wb') as f:\n",
    "        embedding_matrix = np.concatenate(embeddings, axis=0)\n",
    "        np.save(f, embedding_matrix)\n",
    "\n",
    "    chunks_file = os.path.join(embeddings_path, f'chunks-{idx:05d}-of-00041.pkl')\n",
    "    with open(chunks_file, 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    del embedding_matrix\n",
    "    del embeddings\n",
    "    del chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to index embeddings\n",
    "* Join embedding matrices from 41 files, train FAISS index, add to index.\n",
    "* Enter document chunks into a simple database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0 of 41\n",
      "File 1 of 41\n",
      "File 2 of 41\n",
      "File 3 of 41\n",
      "File 4 of 41\n",
      "File 5 of 41\n",
      "File 6 of 41\n",
      "File 7 of 41\n",
      "File 8 of 41\n",
      "File 9 of 41\n",
      "File 10 of 41\n",
      "File 11 of 41\n",
      "File 12 of 41\n",
      "File 13 of 41\n",
      "File 14 of 41\n",
      "File 15 of 41\n",
      "File 16 of 41\n",
      "File 17 of 41\n",
      "File 18 of 41\n",
      "File 19 of 41\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "for idx, file in enumerate(files[0:(len(files)//2)]):\n",
    "    print(f\"File {idx} of 41\")\n",
    "    embeddings_file = os.path.join(embeddings_path, f'embeddings-{idx:05d}-of-00041.npy')\n",
    "    embeddings.append(np.load(embeddings_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = os.path.join(embeddings_path, f'embeddings-matrix-1-of-2.npy')\n",
    "with open(embeddings_file, 'wb') as f:\n",
    "    np.save(f, np.concatenate(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mfiles\u001b[49m[(\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m):\u001b[38;5;241m30\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     file_id \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of 41\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "for idx, file in enumerate(files[(len(files)//2):]):\n",
    "    file_id = idx + len(files)//2\n",
    "    print(f\"File {file_id} of 41\")\n",
    "    embeddings_file = os.path.join(embeddings_path, f'embeddings-{file_id:05d}-of-00041.npy')\n",
    "    embeddings.append(np.load(embeddings_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embeddings_file = os.path.join(embeddings_path, f'embeddings-matrix-2-of-2.npy')\n",
    "with open(embeddings_file, 'wb') as f:\n",
    "    np.save(f, np.concatenate(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert faiss.get_num_gpus() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This type of index is recommended in the FAISS docs for our scale of number of embeddings\n",
    "embed_dim = 384\n",
    "index = faiss.index_factory(embed_dim, \"PCA64,IVF16384_HNSW32,Flat\")\n",
    "index_ivf = faiss.extract_index_ivf(index)\n",
    "clustering_index = faiss.index_cpu_to_all_gpus(faiss.IndexFlatL2(index_ivf.d))\n",
    "index_ivf.clustering_index = clustering_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "embeddings_file = os.path.join(embeddings_path, f'embeddings-{idx:05d}-of-00041.npy')\n",
    "chunks_file = os.path.join(embeddings_path, f'chunks-{idx:05d}-of-00041.pkl')\n",
    "\n",
    "embedding_matrix = np.load(embeddings_file)\n",
    "\n",
    "with open(chunks_file, 'rb') as f:\n",
    "    chunks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1668492, (1668492, 384), dtype('float16'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks), embedding_matrix.shape, embedding_matrix.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
